{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnymy3Sk9vGl"
   },
   "source": [
    "# Wprowadzenie\n",
    "\n",
    "Język flagowy Mojo jest flagowym produktem firmy Modular, zjamującej się tworzeniem narzędzi i technologii dla programistów sztucznej inteligencji. Jego celem jest połączenie wydajności C++ z prostotą i ekosystemem Pythona. Dzięki umożliwieniu programowania zarówno pod CPU oraz akceleratory GPU, znajduje on zastosowanie w uczeniu maszynowym i obliczeniach numerycznych.\n",
    "![Logo Mojo](https://cdn.prod.website-files.com/63f9f100025c058594957cca/65df9332b319cb9989698874_mojo.jpg)\n",
    "![Logo Modular](https://cdn.prod.website-files.com/64174a9fd03969ab5b930a08/642853d18fbcbe9695c3f343_64078db03b0c895dbc658746_mod-word-mark.png)\n",
    "![Logo Python](https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png)\n",
    "\n",
    "### Geneza\n",
    "\n",
    "Twórcą firmy Modular oraz języka Mojo jest Chris Lattner – **_programista odpowiedzialny za stworzenie toolchainu LLVM oraz języka programowania Swift_**(Do zmiany nie podoba mi się to zdanie). Jego prace miały ogromny wpływ na świat kompilatorów i nowoczesnych języków programowania. Mojo to kolejny krok w tej ewolucji – język stworzony specjalnie do wydajnego programowania niskopoziomowego, stricte pod kątem akceleratorów AI takich jak GPU, TPU, NPU itp.\n",
    "\n",
    "![YT](https://i.ytimg.com/vi/-8TbsCUuwQQ/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&rs=AOn4CLCoYhk-zm8iz3C7aiqJJU8NidosJQ)\n",
    "\n",
    "<center style={{ display: 'flex' }}>\n",
    "\n",
    "![LLVM](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRZsvm8rP2Wlxqe3U2uTTbJ5X4MkXeMF57N9w&s)\n",
    "\n",
    "![Swift](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRd4u7UgyLvk9FitFFZsUjP0o__BHUlDzmmeQ&s)\n",
    "\n",
    "</center>\n",
    "\n",
    "#GPU TPU NPU porównanie#\n",
    "\n",
    "Mojo został zaprojektowany, aby połączyć prostotę Pythona z wydajnością C++, a jego głównym celem jest maksymalne wykorzystanie potencjału nowoczesnych układów obliczeniowych. Jest to język przeznaczony do programowania niskopoziomowego, będąc przy tym bardziej przystępny niż tradycyjne rozwiązania.\n",
    "\n",
    "![Logo OpenCl](https://miro.medium.com/v2/resize:fit:910/1*eaiku5FlPeoeStXppsTQzg.png)\n",
    "\n",
    "```\n",
    "// Kod OpenCL przypomina C/C++\n",
    "__kernel void vector_add(__global const float *a,\n",
    "                       __global const float *b,\n",
    "                       __global float *c,\n",
    "                       const unsigned int n) {\n",
    "  int id = get_global_id(0);\n",
    "  if (id < n) {\n",
    "    c[id] = a[id] + b[id];\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "![Logo Nividia Cuda](https://static.wikia.nocookie.net/logopedia/images/1/1f/Nvidia_CUDA.svg/revision/latest?cb=20230319014140)\n",
    "\n",
    "```\n",
    "// Kod CUDA przypomina C/C++\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void vector_add(const float *a, const float *b, float *c, int n) {\n",
    "  int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (id < n) {\n",
    "    c[id] = a[id] + b[id];\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "# Kod Mojo przypomina Python\n",
    "from mojo.vector import simd\n",
    "from tensor import Tensor, TensorShape\n",
    "\n",
    "@simd\n",
    "fn vector_add(a: Tensor[Float32], b: Tensor[Float32], c: Tensor[Float32], n: Int):\n",
    "    for i in range(n):\n",
    "        c[i] = a[i] + b[i]\n",
    "```\n",
    "\n",
    "Jedną z kluczowych zalet Mojo jest jego kompilator, który automatycznie optymalizuje kod pod sprzęt docelowy (target device). Aby to osiągnąć, Mojo korzysta z technologii opracowanej przez Google w ramach LLVM, zwanej Multi-Level Intermediate Representation (MLIR).\n",
    "\n",
    "<center>\n",
    "\n",
    "![Logo MLIR](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSZ1c3YQ7AN8NlrdiTf-xCdcGnZxovlqKQ6QA&s)\n",
    "\n",
    "</center>\n",
    "\n",
    "### Czym jest MLIR?\n",
    "\n",
    "MLIR to rozwiązanie opracowane przez Google w ramach LLVM, które sprowadza kod do wspólnej reprezentacji dla wielu poziomów abstrakcji. Dzięki temu umożliwia:\n",
    "\n",
    "- Konwersję operacji tensorowych i matematycznych na najbardziej efektywne wersje,\n",
    "\n",
    "- Automatyczne dostosowanie kodu do konkretnego sprzętu (scalanie operacji tensorowych dla TPU, czy lepsza paralelizacja dla GPU).\n",
    "\n",
    "MLIR pozwala więc na łatwe i wydajne programowanie akceleratorów, eliminując potrzebę ręcznego dostrajania kodu pod konkretne urządzenia.\n",
    "\n",
    "### Konkretniej jak działa\n",
    "\n",
    "MLIR (Multi-Level Intermediate Representation) działa poprzez sprowadzenie operacji do symbolicznej reprezentacji, co pozwala na analizę i transformację kodu na różnych poziomach abstrakcji. Przykładowo, prosty program w Pythonie, taki jak print(42), może zostać sprowadzony do ciągu instrukcji maszyny wirtualnej, które są bardziej szczegółowe i bliższe niskiemu poziomowi wykonania.\n",
    "\n",
    "```\n",
    "# Załaduj stałą 42 do rejestru R1\n",
    "OP_LOAD_CONST R1, 42\n",
    "# Załaduj funkcję 'print' z globalnego zakresu do rejestru R2\n",
    "OP_LOAD_GLOBAL R2, \"print\"\n",
    "# Wywołaj funkcję 'print' z argumentem w R1\n",
    "OP_CALL R2, [R1]\n",
    "# Zwróć None (wartość domyślna w Pythonie)\n",
    "OP_RETURN None\n",
    "# Zatrzymaj wykonanie\n",
    "OP_STOP\n",
    "```\n",
    "\n",
    "Ten kod, reprezentujący operacje na maszynie wirtualnej, może zostać dalej przetworzony na reprezentację MLIR, która jest bardziej abstrakcyjna i umożliwia głębszą analizę. Operacje te są na tyle fundamentalne, że można je sprowadzić do reprezentacji MLIR, co ułatwia dalszą analizę i transformację kodu.\n",
    "\n",
    "```\n",
    "module @python_print_42 {\n",
    "  func @main() -> !python.object {\n",
    "    %const_42 = python.const 42 : !python.int  // Załaduj stałą 42\n",
    "    %print_func = python.load_global \"print\" : !python.object // Załaduj funkcję 'print'\n",
    "    %result = python.call %print_func(%const_42) : (!python.object, !python.object) -> !python.object // Wywołaj print(42)\n",
    "    return %result : !python.object // Zwróć wynik (None)\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "W MLIR, przestrzeganie pewnych zasad, takich jak Single Static Assignment (SSA), jest kluczowe dla efektywnej analizy i optymalizacji kodu. SSA gwarantuje, że każda zmienna jest przypisywana tylko raz, co ułatwia śledzenie zależności danych i wykonywanie transformacji. Dzięki temu, MLIR może wykonywać zaawansowane analizy, takie jak analiza przepływu danych, analiza zależności oraz optymalizacje, które poprawiają wydajność kodu.\n",
    "\n",
    "### Dialekty\n",
    "\n",
    "Dialekty to konstrukcje reprezentujące operacje specyficzne dla danej domeny. Przykładowym dialektem jest %print_func który reprezentuje \"Coś co printuje\". Poprzez uzupełnienie tych fundamentalnych bloków własnymi ich definicjami, przykładowo\n",
    "\n",
    "```\n",
    "printf // C\n",
    "console.log // JS\n",
    "puts // Ruby\n",
    "Decir // Hiszpański\n",
    "```\n",
    "\n",
    "Co umożliwia przekompilowanie programu z jednego języka programowania na inny, bardziej odpowiadający problemom domenowym, które chcemy rozwiązać. Przykładowo, docelowym językiem może być Intermediate Representation dla LLVM (kod maszynowy) lub NVVM (CUDA). Dzięki tej sztuczce, Mojo może osiągnąć wydajność porównywalną z C++, pisząc program w języku podobnym do Pythona, co jest kluczowe dla wygodnego dostępu do jego bibliotek i abstrakcji.\n",
    "\n",
    "Jest to zdecydowanie przyjemniejsze i wydajniejsze niż korzystanie z Numpy poprzez Serwer HTTP, FFI, czy inną warstwę pośredniczą.\n",
    "\n",
    "### Control Flow Graph\n",
    "\n",
    "Dialekty w MLIR oferują jeszcze jedną potężną możliwość: umożliwiają tworzenie grafów przepływu sterowania (CFG). CFG to reprezentacja grafowa, która pokazuje, jak sterowanie przepływa przez program. Wierzchołki grafu reprezentują bloki kodu, a krawędzie reprezentują możliwe przejścia między tymi blokami. Dzięki tej reprezentacji, MLIR jest w stanie wydedukować i analizować różne ścieżki wykonania programu, co otwiera drzwi do zaawansowanych optymalizacji.\n",
    "\n",
    "![Graf Dialektów](https://lowlevelbits.org/img/compiling-ruby-3/what-is-mlir.png)\n",
    "\n",
    "Istnieje wiele zaawansowanych technik optymalizacji, takich jak fuzja tensorów, która pozwala na łączenie wielu operacji tensorowych w jedną, bardziej złożoną operację. Takie podejście znacząco redukuje narzut związany z uruchamianiem wielu pojedynczych operacji, co jest szczególnie istotne w kontekście obliczeń na GPU. Reprezentacja pośrednia, taka jak ta stosowana w MLIR, ułatwia kompilatorowi wykrycie możliwości fuzji tensorów, ponieważ operacje są reprezentowane w sposób symboliczny i strukturalny. To pozwala na analizę zależności danych i identyfikację sekwencji operacji, które można połączyć.\n",
    "\n",
    "![Tensor Fusion](https://www.researchgate.net/publication/362952579/figure/fig3/AS:11431281256139673@1719510347593/The-encoding-module-based-on-tensor-fusion.jpg)\n",
    "\n",
    "Dodatkowo, reprezentacja pośrednia umożliwia efektywną paralelizację obliczeń na GPU. Dzięki analizie grafu przepływu danych, kompilator może automatycznie generować kod, który wykorzystuje równoległe możliwości GPU. Operacje tensorowe, które są niezależne od siebie, mogą być wykonywane równocześnie na różnych rdzeniach GPU, co znacznie przyspiesza obliczenia. Fuzja tensorów dodatkowo zwiększa ten efekt, ponieważ łączenie operacji zmniejsza liczbę uruchomień jądra GPU, co minimalizuje narzut związany z synchronizacją i przesyłaniem danych.\n",
    "\n",
    "![Paralelizacja](https://bstncdn.net/i/1498)\n",
    "\n",
    "W rezultacie, kompilator, który operuje na odpowiedniej reprezentacji pośredniej, jest w stanie _automatycznie_ wykryć nasze intencje i zastosować zaawansowane optymalizacje, takie jak fuzja tensorów i paralelizacja GPU, co prowadzi do znacznego wzrostu wydajności. To jest szczególnie ważne w kontekście uczenia maszynowego i innych aplikacji, które wymagają intensywnych obliczeń tensorowych. Czyli takich do których stworzony został język Mojo.\n",
    "\n",
    "### Ale dlaczego Python??\n",
    "\n",
    "Mimo swoich licznych wad, niepodważalną zaletą pythona jest ogromny i dojrzały ekosystem. Biblioteki do ML/AI, takie jak TensorFlow, PyTorch, oraz do big data, takie jak Pandas, PySpark sprawiają, że stał się standardem w tych dziedzinach. Mojo stara się czerpać z tej siły, ale jednocześnie usuwa ograniczenia wydajnościowe Pythona, pozwalając na jego wykorzystanie w programowaniu niskopoziomowym.\n",
    "\n",
    "Dodatkowo, jego minimalistyczna i zwięzła forma idealnie nadaje się do analizy i optymalizacji przez MLIR.\n",
    "\n",
    "![logo Pytorch](https://miro.medium.com/v2/resize:fit:1200/1*r2eKvfvYPQuizKLOh9q7Hw.jpeg)\n",
    "\n",
    "<center>\n",
    "\n",
    "![Logo Pandas](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR-4t7O82hSHWJNpROgVI3ae7dcrRfEHUNoRg&s)\n",
    "\n",
    "![Logo Pip](https://miro.medium.com/v2/resize:fit:300/0*lmWLpGJfUo-OnpC2.jpg)\n",
    "\n",
    "</center>\n",
    "\n",
    "Mojo jest skierowane do programistów i naukowców, którzy potrzebują wydajności zbliżonej do C++, ale nie są gotowi rezygnować z znajomej prostoty i ekosystemu pythona. Są to między innymi\n",
    "\n",
    "- Programiści AI/ML: Tworzenie wydajnych modeli uczenia maszynowego i aplikacji AI.\n",
    "- Naukowców danych: Analiza dużych zbiorów danych z wykorzystaniem bibliotek Pythona, ale z większą wydajnością.\n",
    "- Programiści systemów: Tworzenie niskopoziomowych aplikacji, które wymagają wysokiej wydajności.\n",
    "- Ogół programistów pythona: Chcących pisać kod Pythona, który jest szybszy i bardziej wydajny.\n",
    "\n",
    "Mojo ma na celu wypełnienie luki między łatwością użycia Pythona a wydajnością C++, co czyni go atrakcyjnym wyborem dla szerokiego grona programistów.\n",
    "\n",
    "### Alternatywy\n",
    "\n",
    "W kontekście Pythona istnieje już kilka alternatywnych podejść, które próbują rozwiązać problem wydajności, choć każde z nich robi to w nieco inny sposób. Oto kilka z nich i porównanie z Mojo.\n",
    "Mojo, jako nowy język programowania, który łączy łatwość użycia Pythona z wydajnością C++, stawia sobie ambitny cel: wypełnienie luki między tymi dwoma światami. W kontekście Pythona istnieje już kilka alternatywnych podejść, które próbują rozwiązać problem wydajności, choć każde z nich robi to w nieco inny sposób. Oto kilka z nich i porównanie z Mojo:\n",
    "\n",
    "1. Cython - język programowania, który również jest nadzbiorem Pythona. Pozwala na pisanie kodu, który jest kompilowany do C, co znacznie przyspiesza jego wykonanie. Jest szeroko stosowany do optymalizacji istniejących bibliotek Pythona, takich jak NumPy.\n",
    "   Porównanie z Mojo:\n",
    "\n",
    "- Cython wymaga nauki dodatkowej, specyficznej składni i ręcznej optymalizacji. Efektywne go wykorzystanie jest czasochłonne i wymaga zaawansowanej wiedzy oraz lat doświadczenia\n",
    "\n",
    "- Mojo oferuje głębszą integrację z MLIR, dzięki czemu jest w stanie osiągnąć lepszą wydajność. Wykorzystuje do tego analizę grafu przepływu by wprowadzić zaawansowane optymalizacje w miejscach, w których nawet najbardziej doświadczeni programiści zawiodą.\n",
    "\n",
    "<center>\n",
    "\n",
    "![Logo Cython](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT3k74gA2JM9rFe0C7_GrV2xB03X_jm_6Z87w&s)\n",
    "\n",
    "</center>\n",
    "\n",
    "2. Numba - kompilator just-in-time (JIT) dla Pythona, który tłumaczy kod Pythona na zoptymalizowany kod maszynowy w czasie wykonywania.\n",
    "   Jest szczególnie skuteczny w optymalizacji obliczeń numerycznych i operacji na macierzach, co jest kluczowe w uczeniu maszynowym. Porównanie z Mojo:\n",
    "\n",
    "- Numba działa w czasie wykonywania, co może powodować pewien narzut związany z kompilacją JIT. Mojo kompiluje kod statycznie, co pozwala na lepsze optymalizacje w czasie kompilacji i eliminuje narzut w czasie wykonywania.\n",
    "\n",
    "![Logo Numba](https://inlocrobotics.com/wp-content/uploads/2021/06/numba-1.jpg)\n",
    "\n",
    "3. PyPy - alternatywna implementacja Pythona, która wykorzystuje kompilator JIT do przyspieszenia wykonywania kodu. Jest szczególnie skuteczny w optymalizacji kodu, który wykonuje wiele operacji dynamicznych, takich jak operacje na listach i słownikach. Porównanie z Mojo:\n",
    "\n",
    "- PyPy jest kompatybilny z istniejącym kodem Pythona, ale nie oferuje tak dużych możliwości optymalizacji niskopoziomowej jak Mojo, celującego w umożliwienie programowania niskopoziomowego z zachowaniem prostoty Pythona, co pozwala na pisanie kodu, który jest zarówno wydajny, jak i czytelny.\n",
    "\n",
    "![Logo PyPy](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5d/Pypy-logo_%282020%29.svg/2560px-Pypy-logo_%282020%29.svg.png)\n",
    "\n",
    "4. JAX - biblioteka Pythona do transformacji programów numerycznych.\n",
    "   umożliwia automatyczną dyferencjację, wektoryzację i kompilację kodu Pythona do kodu przyspieszonego na GPU/TPU. Porównanie z Mojo:\n",
    "\n",
    "- Mimo że JAX jest imponujący w swojej dziedzinie, jest ograniczony byciem jedynie biblioteką w ekosystemie Pythona. Oznacza to, że jego możliwości są ograniczone do transformacji programów numerycznych i nie oferuje pełnej kontroli nad niskopoziomowymi aspektami programowania, jak robi to Mojo. Efektywne użycie Mojo nie wymaga czasu spędzonego na optymalizacji i dogłębnej wiedzy o transformacjach numerycznych, osiągając przy tym nie gorsze rezultaty.\n",
    "\n",
    "![Logo JAX](https://preview.redd.it/the-jax-logo-v0-mcrezb1fy9bb1.png?auto=webp&s=81b1cd55dd5b0580081bf2bb188dc65849a101c7)\n",
    "\n",
    "# W praktyce\n",
    "\n",
    "Mojo jest aktualnie dostępny na systemach Linux oraz MacOS. Eksperymentalne wsparcie systemu Windows oparte jest na zintegrowanym środowisku Windows Subsystem for Linux (Nam nie udało się na nim wiele zdziałać). Skorzystać z Mojo możemy poprzez narzędzie CLI Magic, które służy do interakcji z produktami firmy Modular. Jego instalacja sprowadza się do wykonania zapytania CURL pod odpowiedni adres (Jest on generowany automatycznie po wejściu w dokumentację)\n",
    "\n",
    "#Linux logo#\n",
    "#MacOS logo#\n",
    "#WSL logo tylko przekreślone bo to był bolesny żart#\n",
    "\n",
    "```\n",
    "curl -ssL https://magic.modular.com/deb13af9-76a7-4aa4-b9a3-98fc64f58c8e | bash\n",
    "```\n",
    "\n",
    "### Hello World\n",
    "\n",
    "Aby utworzyć najprostszy program, potrzebujemy zainicjować środowisko wirtualne, działające analogicznie do tych z ekosystemu pythona\n",
    "\n",
    "```bash\n",
    "magic init #NAZWA PROJEKTU# --format mojoproject\n",
    "```\n",
    "\n",
    "Do uruchomienia programu użyjemy jednej z dwóch komend:\n",
    "\n",
    "- magic run -- Uruchamia wskazany program.\n",
    "- magic build -- Tworzy plik wykonywalny z projektu.\n",
    "\n",
    "Kod wykonywany znajdzie się w funkcji o nazwie main (analogicznie w C++)\n",
    "\n",
    "```mojo\n",
    "fn main():\n",
    "  print(\"Hello, world\")\n",
    "```\n",
    "\n",
    "## System typów w Mojo\n",
    "\n",
    "Jednym z filarów Mojo jest jego elastyczny system typów, który pozwala na balansowanie między wygodą dynamicznego typowania (jak w Pythonie), a bezpieczeństwem i wydajnością statycznego typowania (jak w C++).\n",
    "\n",
    "Mojo umożliwia używanie:\n",
    "\n",
    "- Typów statycznych (`Int`, `Float64`, `Bool`, `Struct`) — wykorzystywanych do maksymalizacji wydajności\n",
    "- Typów dynamicznych — przez pełną interoperacyjność z Pythonem\n",
    "- Typów generowanych w czasie kompilacji (`let`, `var`, `const`) — umożliwiają precyzyjne kontrolowanie mutowalności\n",
    "\n",
    "```mojo\n",
    "fn add(a: Int, b: Int) -> Int:\n",
    "    return a + b\n",
    "\n",
    "```\n",
    "\n",
    "### Przykład 1: Fibbonacci\n",
    "\n",
    "```py\n",
    "from python import Python\n",
    "from tensor import Tensor, TensorShape\n",
    "from python import PythonObject\n",
    "\n",
    "# Mnożenie macierzy 2x2\n",
    "fn mat_mult(A: Tensor[DType.int64], B: Tensor[DType.int64]) -> Tensor[DType.int64]:\n",
    "    var t = Tensor[DType.int64]((2,2))\n",
    "    t.store(0, A[0,0] * B[0,0] + A[0,1] * B[1,0])\n",
    "    t.store(1, A[0,0] * B[0,1] + A[0,1] * B[1,1])\n",
    "    t.store(2, A[1,0] * B[0,0] + A[1,1] * B[1,0])\n",
    "    t.store(3, A[1,0] * B[0,1] + A[1,1] * B[1,1])\n",
    "    return t\n",
    "\n",
    "# Potęgowanie macierzy\n",
    "fn mat_pow(Q: Tensor[DType.int64], n: Int) -> Tensor[DType.int64]:\n",
    "    var result = Tensor[DType.int64]((2, 2))\n",
    "    result.store(0, 1)\n",
    "    result.store(3, 1)\n",
    "    var base = Q\n",
    "\n",
    "    var exp = n\n",
    "    while exp > 0:\n",
    "        if exp % 2 == 1:\n",
    "            result = mat_mult(result, base)\n",
    "        base = mat_mult(base, base)\n",
    "        exp = exp // 2\n",
    "\n",
    "    return result\n",
    "\n",
    "# Tworzenie macierzy Q\n",
    "def get_fib_mat() -> Tensor[DType.int64]:\n",
    "  var t = Tensor[DType.int64]((2, 2))\n",
    "  t.store(0, 1)\n",
    "  t.store(1, 1)\n",
    "  t.store(2, 1)\n",
    "\n",
    "  return t\n",
    "\n",
    "\n",
    "def main():\n",
    "  plt = Python.import_module(\"matplotlib.pyplot\")\n",
    "  a = get_fib_mat()\n",
    "  x = Python.evaluate(\"[]\")\n",
    "  y = Python.evaluate(\"[]\")\n",
    "  outer_n = -1\n",
    "  for n in range(1, 200):\n",
    "    v = mat_pow(a, n)[1].__int__()\n",
    "    if (v < 0):\n",
    "      # Integer overflow\n",
    "      outer_n = n\n",
    "      break\n",
    "    x.append(n)\n",
    "    y.append(v)\n",
    "\n",
    "  print(outer_n, \"liczba fibonacciego powoduje integer overflow\")\n",
    "  plt.plot(x, y)\n",
    "  plt.show()\n",
    "```\n",
    "\n",
    "Widoczne różnice:\n",
    "\n",
    "- Deklarowane zmienne muszą posiadać typy.\n",
    "- Operujemy najczęściej na mało wygodnch Tensorach\n",
    "- Mojo ma własny zestaw bibliotek, zależności pythona możemy importować korzystając z modułu Python.\n",
    "\n",
    "## Porównanie z CUDA\n",
    "\n",
    "CUDA wymaga znajomości C/C++ oraz specyficznych konstrukcji NVIDIA, co tworzy wysoką barierę wejścia dla specjalistów data science, którzy zazwyczaj mają doświadczenie tylko w Pythonie. Dodatkowo działa tylko na kartach NVIDIA.\n",
    "\n",
    "Mojo oferuje składnie podobną do Pythona, zachowując przy tym możliwość mikrozarządzania pamięcią i wydajnością. Dzięki MLIR może kierować kod na różne akceleratory (GPU, TPU, NPU) bez zmian w kodzie źródłowym.\n",
    "\n",
    "### Przykład - Dodawanie elementów macierzy\n",
    "\n",
    "```c\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void addKernel(int *c, const int *a, const int *b, int size) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i < size)\n",
    "        c[i] = a[i] + b[i];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    const int size = 5;\n",
    "    int a[size] = {1, 2, 3, 4, 5};\n",
    "    int b[size] = {10, 20, 30, 40, 50};\n",
    "    int c[size] = {0};\n",
    "\n",
    "    // Allocate GPU memory\n",
    "    int *dev_a, *dev_b, *dev_c;\n",
    "    cudaMalloc(&dev_a, size * sizeof(int));\n",
    "    cudaMalloc(&dev_b, size * sizeof(int));\n",
    "    cudaMalloc(&dev_c, size * sizeof(int));\n",
    "\n",
    "    // Copy inputs to GPU\n",
    "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    addKernel<<<1, size>>>(dev_c, dev_a, dev_b, size);\n",
    "\n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    for(int i = 0; i < size; i++)\n",
    "        printf(\"%d + %d = %d\\n\", a[i], b[i], c[i]);\n",
    "\n",
    "    cudaFree(dev_a); cudaFree(dev_b); cudaFree(dev_c);\n",
    "}\n",
    "```\n",
    "\n",
    "- \\_\\_global\\_\\_ - specyfikator oznaczający, że funkcja jest kernelem wykonywanym na GPU\n",
    "- blockIdx, blockDim, threadIdx - zmienne wbudowane określające pozycję wątku w hierarchii GPU\n",
    "- cudaMalloc - alokuje pamięć na GPU. Przyjmuje adres wskaźnika (stąd &dev_a) i rozmiar do zaalokowania\n",
    "- cudaMemcpy - kopiuje dane między pamięcią CPU i GPU\n",
    "- <<<numBlocks, threadsPerBlock>>> - składnia uruchamiania kernela z konfiguracją wątków\n",
    "- cudaFree - zwalnia pamięć zaalokowaną na GPU (podobnie jak free() dla standardowej pamięci CPU).\n",
    "\n",
    "```py\n",
    "from memory import UnsafePointer\n",
    "from algorithm import parallelize\n",
    "\n",
    "alias size = 5\n",
    "alias type = DType.int32\n",
    "\n",
    "fn main():\n",
    "    # Dynamic arrays in Mojo, that have to be freed\n",
    "    var a = UnsafePointer[Scalar[type]].alloc(size)\n",
    "    var b = UnsafePointer[Scalar[type]].alloc(size)\n",
    "    var c = UnsafePointer[Scalar[type]].alloc(size)\n",
    "\n",
    "    @parameter\n",
    "    fn init_arrays(i: Int):\n",
    "        a.store(i, i + 1)\n",
    "        b.store(i, (i + 1) * 10)\n",
    "        c.store(i, 0)\n",
    "    parallelize[init_arrays](size, size)\n",
    "\n",
    "    # Define the parallel addition function (simulating CUDA kernel)\n",
    "    @parameter\n",
    "    fn add_vectors(i: Int):\n",
    "        c.store(i, a.load(i) + b.load(i))\n",
    "\n",
    "    # Execute the parallel computation (similar to launching a CUDA kernel)\n",
    "    parallelize[add_vectors](size, size)\n",
    "\n",
    "    # Print results directly from the pointers\n",
    "    for i in range(size):\n",
    "        print(a.load(i), \"+\", b.load(i), \"=\", c.load(i))\n",
    "\n",
    "    a.free(); b.free(); c.free();\n",
    "```\n",
    "\n",
    "- UnsafePointer - pozwala na bezpośrednie zarządzanie pamięcią (podobnie jak wskaźniki w C++)\n",
    "- parallelize - funkcja, która umożliwia równoległe wykonywanie kodu (podobnie jak CUDA)\n",
    "- @parameter - dekorator funkcji, który oznacza, że funkcja może być przekazana jako parametr funkcji wyższego rzędu\n",
    "\n",
    "## Benchmark: Mojo vs Python\n",
    "\n",
    "Mojo celuje w wydajność zbliżoną do C. Przykład prostego benchmarku pokazuje różnicę między Pythonem a Mojo dla funkcji rekurencyjnej:\n",
    "\n",
    "Python:\n",
    "```py\n",
    "def fib(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fib(n-1) + fib(n-2)\n",
    "\n",
    "%timeit fib(20)\n",
    "```\n",
    "\n",
    "Mojo:\n",
    "```mojo\n",
    "from time import time\n",
    "\n",
    "fn fib(n: Int) -> Int:\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fib(n - 1) + fib(n - 2)\n",
    "\n",
    "fn main():\n",
    "    let n = 20\n",
    "\n",
    "    let start = time()\n",
    "    let result = fib(n)\n",
    "    let end = time()\n",
    "\n",
    "    print(\"fib(\", n, \") = \", result)\n",
    "    print(\"Czas wykonania: \", end - start, \" sekund\")\n",
    "```\n",
    "\n",
    "Według wczesnych testów, Mojo może być **nawet 30-60x szybsze** od Pythona w takich przypadkach, ponieważ kod jest kompilowany do niskopoziomowej reprezentacji i nie korzysta z dynamicznej maszyny wirtualnej.\n",
    "\n",
    "\n",
    "### Automatyczna optymalizacja\n",
    "\n",
    "- CUDA wymaga ręcznego zarządzania pamięcią, planowania wątków i optymalizacji - programista musi dokładnie rozumieć architekturę GPU\n",
    "- Mojo dzięki MLIR automatycznie optymalizuje kod pod docelowy akcelerator, wykonując fuzję tensorów i paralelizację\n",
    "\n",
    "## Doświadczenie\n",
    "\n",
    "Wygląd może mylić - to nie jest Python. Każda operacja wymaga przemyślenia i świadomości co się dzieje na niskim poziomie. Choć składnia Mojo jest podobna do Pythona, programowanie w tym języku wymaga zupełnie innego podejścia. Programista musi być świadomy:\n",
    "\n",
    "- Zarządzania pamięcią - kiedy alokować i zwalniać zasoby\n",
    "- Typów danych - statyczne typowanie jest w Mojo kluczowe dla wydajności\n",
    "- Przepływu danych - optymalne wykorzystanie GPU/TPU wymaga świadomego projektowania przetwarzania danych\n",
    "- Równoległości - wiele operacji może być wykonywanych jednocześnie, co wymaga odpowiedniego planowania\n",
    "\n",
    "Społeczność Mojo jest wciąż w fazie rozwoju. Dokumentacja jest nierówna - niektóre części są szczegółowo opisane, inne pozostają wybiórczo opracowane lub wręcz pominięte. Ponieważ język jest w fazie intensywnego rozwoju, trudno znaleźć przykłady które działają bez konieczności wprowadzania licznych modyfikacji. Większość materiałów edukacyjnych pochodzi bezpośrednio od firmy Modular, a liczba tutoriali stworzonych przez społeczność jest nadal ograniczona.\n",
    "\n",
    "Platformy do wymiany wiedzy, takie jak StackOverflow czy GitHub, mają niewiele pytań i przykładów kodu dla Mojo w porównaniu z dojrzałymi językami. Większość problemów trzeba rozwiązywać samodzielnie, eksperymentując z kodem i analizując dostępną dokumentację.\n",
    "\n",
    "## Zastosowania\n",
    "\n",
    "Mojo znajduje zastosowanie przede wszystkim w obszarach wymagających intensywnych obliczeń i wysokiej wydajności:\n",
    "Uczenie maszynowe\n",
    "\n",
    "### Trenowanie modeli neural networks z wykorzystaniem akceleratorów GPU/TPU\n",
    "\n",
    "- Implementacja własnych warstw i operatorów dla frameworków ML\n",
    "- Optymalizacja inferencji modeli w czasie rzeczywistym\n",
    "- Przetwarzanie danych treningowych z wysoką wydajnością\n",
    "\n",
    "### Przetwarzanie obrazów i wideo\n",
    "\n",
    "- Filtrowanie i transformacja obrazów w czasie rzeczywistym\n",
    "- Analiza wideo dla systemów wizji komputerowej\n",
    "- Generowanie obrazów (np. w modelach generatywnych)\n",
    "- Akceleracja algorytmów wykrywania i śledzenia obiektów\n",
    "\n",
    "### Obliczenia numeryczne\n",
    "\n",
    "- Symulacje fizyczne (dynamika płynów, mechanika kwantowa)\n",
    "- Obliczenia macierzowe na dużą skalę\n",
    "- Rozwiązywanie złożonych równań różniczkowych\n",
    "- Analiza statystyczna dużych zbiorów danych\n",
    "\n",
    "## Mojo w sztucznej inteligencji\n",
    "\n",
    "Mojo został zaprojektowany z myślą o zastosowaniach AI i ML — zarówno do pisania wysokopoziomowej logiki, jak i optymalizacji niskopoziomowych kernelów dla akceleratorów (GPU, TPU, NPU).\n",
    "\n",
    "Zalety:\n",
    "- Bezpośrednia kontrola nad alokacją i przenoszeniem danych do akceleratorów\n",
    "- Kompatybilność z bibliotekami Pythona (np. NumPy, TensorFlow)\n",
    "- Możliwość pisania własnych operacji dla modeli AI, które są potem ekstremalnie szybkie\n",
    "\n",
    "Przykładowe zastosowania:\n",
    "- Pisanie niestandardowych warstw neuronowych\n",
    "- Optymalizacja obliczeń macierzowych\n",
    "- Inference modeli na edge-devices bez Pythonowego runtime'u\n",
    "\n",
    "Mojo umożliwia np. stworzenie własnego kernelu do mnożenia macierzy działającego szybciej niż rozwiązania obecne w NumPy.\n",
    "\n",
    "## Ownership system z Rusta\n",
    "\n",
    "Jedną z najbardziej innowacyjnych cech Mojo jest wprowadzenie systemu własności (ownership) podobnego do tego znanego z języka Rust. System ten zapewnia bezpieczeństwo pamięci bez konieczności korzystania z garbage collectora, co jest kluczowe dla wysokowydajnych zastosowań.\n",
    "\n",
    "Główne zasady systemu własności w Mojo:\n",
    "\n",
    "- Każda wartość ma dokładnie jednego właściciela\n",
    "- Kiedy właściciel wychodzi poza zakres, wartość jest automatycznie dealokowana\n",
    "- Wartości mogą być pożyczone (borrowed) przez referencje, ale właściciel nie może ich modyfikować dopóki są pożyczone\n",
    "\n",
    "Przykładowy kod demonstrujący system własności:\n",
    "\n",
    "```py\n",
    "from memory import UnsafePointer\n",
    "\n",
    "fn borrow_example():\n",
    "    var my_string = String(\"Hello Mojo\")\n",
    "    var ptr = UnsafePointer[String].alloc(1)\n",
    "\n",
    "    # Move string ownership to ptr\n",
    "    ptr.init_pointee_move(my_string^)\n",
    "\n",
    "    # Attempting to use my_string here would cause a compile-time error\n",
    "    # because its ownership has been moved\n",
    "    # print(my_string)\n",
    "\n",
    "    print(ptr[])  # Prints: Hello Mojo\n",
    "\n",
    "    # Destroy the pointee explicitly before freeing memory\n",
    "    ptr.destroy_pointee()\n",
    "    ptr.free()\n",
    "\n",
    "fn main():\n",
    "    borrow_example()\n",
    "```\n",
    "\n",
    "## Wskaźniki\n",
    "\n",
    "Mojo, mimo swojej wysokopoziomowej składni, oferuje pełny dostęp do niskopoziomowych mechanizmów zarządzania pamięcią poprzez wskaźniki. Możliwość ta jest kluczowa dla optymalizacji wydajności w krytycznych fragmentach kodu.\n",
    "\n",
    "### Mojo oferuje kilka rodzajów wskaźników:\n",
    "\n",
    "- UnsafePointer - podstawowy wskaźnik bez żadnych zabezpieczeń\n",
    "- Pointer - wskaźnik z podstawowymi zabezpieczeniami\n",
    "- OwnedPointer - wskaźnik z automatycznym zarządzaniem czasem życia\n",
    "\n",
    "Przykład użycia wskaźników:\n",
    "\n",
    "```py\n",
    "from memory import UnsafePointer, Pointer\n",
    "\n",
    "fn unsafe_pointer_example():\n",
    "    var size = 5\n",
    "    var ptr = UnsafePointer[Int32].alloc(size)\n",
    "\n",
    "    for i in range(size):\n",
    "        ptr.store(i, i * 10)\n",
    "\n",
    "    for i in range(size):\n",
    "        print(\"Value at index\", i, \":\", ptr.load(i))\n",
    "\n",
    "    ptr.free()\n",
    "\n",
    "fn safe_pointer_example():\n",
    "    var data = 42\n",
    "    var ptr = Pointer.address_of(data)\n",
    "\n",
    "    print(ptr[])\n",
    "\n",
    "fn main():\n",
    "    unsafe_pointer_example()\n",
    "    safe_pointer_example()\n",
    "```\n",
    "\n",
    "Dzięki wskaźnikom Mojo pozwala na:\n",
    "\n",
    "- Bezpośredni dostęp do pamięci akceleratorów (GPU, TPU)\n",
    "- Optymalizację transferu danych między CPU a urządzeniami\n",
    "- Tworzenie własnych struktur danych zoptymalizowanych pod kątem wydajności\n",
    "- Integrację z bibliotekami napisanymi w C/C++\n",
    "\n",
    "Wskaźniki w Mojo zapewniają elastyczność charakterystyczną dla języków niskopoziomowych, jednocześnie zachowując czytelność i bezpieczeństwo dzięki systemowi typów i właściwości.\n",
    "\n",
    "## Zarządzanie pamięcią\n",
    "\n",
    "W przeciwieństwie do Pythona, Mojo **nie posiada garbage collectora**. Zamiast tego, programista ma większą kontrolę nad zarządzaniem pamięcią, podobnie jak w C/C++ czy Rust.\n",
    "\n",
    "Dzięki temu:\n",
    "\n",
    "- Można pisać bardziej przewidywalny i wydajny kod\n",
    "- Łatwiej jest debugować zachowania związane z pamięcią\n",
    "- Oprogramowanie może być używane w systemach wbudowanych lub na urządzeniach o ograniczonych zasobach (IoT, edge AI)\n",
    "\n",
    "To podejście idealnie sprawdza się w systemach, gdzie czas wykonania i deterministyczność mają kluczowe znaczenie — np. przy inferencji modeli AI w czasie rzeczywistym.\n",
    "\n",
    "## Roadmapa Mojo i aktualny status\n",
    "\n",
    "Mojo jest wciąż rozwijany i dostępny obecnie przez **Mojo Playground** – specjalne środowisko przeglądarkowe dostępne po zapisaniu się do testów (https://www.modular.com/mojo).\n",
    "\n",
    "### Aktualne ograniczenia:\n",
    "- Brak możliwości lokalnej instalacji kompilatora (stan na 2025)\n",
    "- Brak w pełni rozwiniętego systemu paczek\n",
    "- Brak dokumentacji pełnej wersji języka (dostępny tylko subset)\n",
    "\n",
    "### Planowane funkcje:\n",
    "- Integracja z popularnymi IDE (VS Code, PyCharm)\n",
    "- Pełna lokalna kompilacja i integracja z istniejącymi projektami ML\n",
    "- System paczek i menedżer zależności\n",
    "- Obsługa większej liczby backendów sprzętowych (CPU, GPU, FPGA)\n",
    "\n",
    "Mojo ma potencjał stać się \"językiem Pythona 2.0\" dla AI, oferując nowy standard w wydajności i skalowalności.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
